{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7e4d69-8caa-4874-8cd9-2fa16f0dbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "import random\n",
    "import splitfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c039b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.iou_scores = []\n",
    "        self.val_losses = []\n",
    "        self.val_iou_scores = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.iou_scores.append(logs.get('iou_score'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.val_iou_scores.append(logs.get('val_iou_score'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34e61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_history = MetricHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2ca82f-fee2-4c81-95a3-3cd36d2227f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget https://landcover.ai/download/landcover.ai.v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9d6136-74e7-4c27-bc6b-1bf15a48f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/caldera/projects/usgs/eros/users/rfleckenstein/data'\n",
    "patch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fa3630-0ba9-4d76-9d47-8d094823b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read images from repsective 'images' subdirectory\n",
    "#As all images are of different size we have 2 options, either resize or crop\n",
    "#But, some images are too large and some small. Resizing will change the size of real objects.\n",
    "#Therefore, we will crop them to a nearest size divisible by 256 and then \n",
    "#divide all images into patches of 256x256x3. \n",
    "img_dir=root_directory+\"/images\"\n",
    "for path, subdirs, files in os.walk(img_dir):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "    #print(dirname)\n",
    "    images = os.listdir(path)  #List of all image names in this subdirectory\n",
    "    #print(images)\n",
    "    for i, image_name in enumerate(images):  \n",
    "        if image_name.endswith(\".tif\"):\n",
    "            #print(image_name)\n",
    "            image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
    "            SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "            SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "            image = Image.fromarray(image)\n",
    "            image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "            #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "            image = np.array(image)             \n",
    "\n",
    "            #Extract patches from each image\n",
    "            #print(\"Now patchifying image:\", path + \"/\" + image_name)\n",
    "            patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)  \n",
    "\n",
    "            for i in range(patches_img.shape[0]):\n",
    "                for j in range(patches_img.shape[1]):\n",
    "\n",
    "                    single_patch_img = patches_img[i,j,:,:]\n",
    "                    single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "\n",
    "                    cv2.imwrite('/caldera/projects/usgs/eros/users/rfleckenstein/512_patches/images'+\n",
    "                            image_name[:-4] + \"patch_\" + str(i) + str(j) + \".tif\", single_patch_img)\n",
    "                    #image_dataset.append(single_patch_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb04fc05-3f9f-4469-acef-2ba8ab35eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do the same as above for masks\n",
    "#For this specific dataset we could have added masks to the above code as masks have extension png\n",
    "mask_dir=root_directory+\"/masks\"\n",
    "for path, subdirs, files in os.walk(mask_dir):\n",
    "    #print(path)  \n",
    "    dirname = path.split(os.path.sep)[-1]\n",
    "\n",
    "    masks = os.listdir(path)  #List of all image names in this subdirectory\n",
    "    for i, mask_name in enumerate(masks):  \n",
    "        if mask_name.endswith(\".tif\"):           \n",
    "            mask = cv2.imread(path + \"/\" + mask_name, 0)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "            SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "            SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "            mask = Image.fromarray(mask)\n",
    "            mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "            #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "            mask = np.array(mask)             \n",
    "\n",
    "            #Extract patches from each image\n",
    "            #print(\"Now patchifying mask:\", path+\"/\"+ mask_name)\n",
    "            patches_mask = patchify(mask, (patch_size, patch_size), step=patch_size)  \n",
    "\n",
    "            for i in range(patches_mask.shape[0]):\n",
    "                for j in range(patches_mask.shape[1]):\n",
    "                    single_patch_mask = patches_mask[i,j,:,:]\n",
    "                    #single_patch_mask = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "                    single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.\n",
    "                    cv2.imwrite('/caldera/projects/usgs/eros/users/rfleckenstein/data/512_patches/masks'+\n",
    "                            mask_name[:-4] + \"patch_\" + str(i) + str(j) + \".tif\", single_patch_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebaafd7-129c-4ad9-b5e2-c796d381f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dir = '/caldera/projects/usgs/eros/users/rfleckenstein/data/512_patches/images'\n",
    "train_mask_dir = '/caldera/projects/usgs/eros/users/rfleckenstein/data/512_patches/masks'\n",
    "\n",
    "img_list = os.listdir(train_img_dir)\n",
    "msk_list = os.listdir(train_mask_dir)\n",
    "\n",
    "num_images = len(os.listdir(train_img_dir))\n",
    "\n",
    "img_num = random.randint(0, num_images-1)\n",
    "\n",
    "img_for_plot = cv2.imread(os.path.join(train_img_dir, img_list[img_num]), 1)\n",
    "\n",
    "img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "mask_for_plot =cv2.imread(os.path.join(train_mask_dir, msk_list[img_num]), 0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_for_plot)\n",
    "plt.title('Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask_for_plot, cmap='gray')\n",
    "plt.title('Mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216f74ba-98bd-4cd1-88ed-120d8c1d1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/caldera/projects/usgs/eros/users/rfleckenstein/data/512_patches'\n",
    "output_folder = '/caldera/projects/usgs/eros/users/rfleckenstein/data/train_val_data'\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.80, .20), group_prefix=None) # default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1156dec-0fd6-4397-bed3-552f28fe3b4e",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b215e237-2a35-41a8-a942-0f8ee603eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dir=root_directory+\"/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19cbe38b-d4ac-4a55-85c5-33db14efbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = img_dir + '/'+ os.listdir(img_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdcb7be3-ba56-4226-9f69-12cbf2f5f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e74868b-ba7a-42cd-a895-65f9d3ce340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dir=root_directory+\"/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b471176-5d4e-41d7-9e5f-f580c234c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = mask_dir + '/'+os.listdir(mask_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca326166-0991-4916-8cda-fb866cf62e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e5a16c1-8052-4d3a-8564-941dbce4cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_for_plot = cv2.imread(image, 1)\n",
    "\n",
    "# img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "# mask_for_plot =cv2.imread(mask, 0)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(img_for_plot)\n",
    "# plt.title('Image')\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(mask_for_plot, cmap='gray')\n",
    "# plt.title('Mask')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89b7d561-261f-4524-be86-8239cb5cc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "315badd8-cfe8-4b54-93bf-0817b4f20cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(image, 1)  #Read each image as BGR\n",
    "# SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "# SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "# image = Image.fromarray(image)\n",
    "# image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "# #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "# image = np.array(image)             \n",
    "\n",
    "# #Extract patches from each image\n",
    "# #print(\"Now patchifying image:\", path + \"/\" + image_name)\n",
    "# patches_img = patchify(image, (256, 256, 3), step=256)  #Step=256 for 256 patches means no overlap\n",
    "\n",
    "# for i in range(patches_img.shape[0]):\n",
    "#     for j in range(patches_img.shape[1]):\n",
    "\n",
    "#         single_patch_img = patches_img[i,j,:,:]\n",
    "#         single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.                               \n",
    "\n",
    "#         cv2.imwrite('/home/jovyan/opt/Image_Seg_practice/data/testing/images/'+ \"patch_\" + str(i) + str(j) + \".tif\", single_patch_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9601bc1b-2d64-4ca8-9ab8-ee126ad0c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = cv2.imread(mask, 0)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "# SIZE_X = (mask.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "# SIZE_Y = (mask.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size\n",
    "# mask = Image.fromarray(mask)\n",
    "# mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "# #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "# mask = np.array(mask)             \n",
    "\n",
    "# #Extract patches from each image\n",
    "# #print(\"Now patchifying mask:\", path+\"/\"+ mask_name)\n",
    "# patches_mask = patchify(mask, (256, 256), step=256)  #Step=256 for 256 patches means no overlap\n",
    "\n",
    "# for i in range(patches_mask.shape[0]):\n",
    "#     for j in range(patches_mask.shape[1]):\n",
    "#         single_patch_mask = patches_mask[i,j,:,:]\n",
    "#         #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "#         #single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.\n",
    "#         cv2.imwrite('/home/jovyan/opt/Image_Seg_practice/data/testing/masks/'+ \"patch_\" + str(i) + str(j) + \".tif\", single_patch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f81cef7-4441-481b-a8b4-b4d5a5a4fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_dir = os.path.join(os.getcwd(), 'data', 'testing', 'images')\n",
    "# train_mask_dir = os.path.join(os.getcwd(), 'data', 'testing', 'masks')\n",
    "\n",
    "# img_list = os.listdir(train_img_dir)\n",
    "# msk_list = os.listdir(train_mask_dir)\n",
    "\n",
    "# num_images = len(os.listdir(train_img_dir))\n",
    "\n",
    "# img_num = random.randint(0, num_images-1)\n",
    "\n",
    "# img_for_plot = cv2.imread(os.path.join(train_img_dir, img_list[img_num]), 1)\n",
    "\n",
    "# img_for_plot = cv2.cvtColor(img_for_plot, cv2.COLOR_BGR2RGB)\n",
    "# mask_for_plot =cv2.imread(os.path.join(train_mask_dir, msk_list[img_num]), 0)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(img_for_plot)\n",
    "# plt.title('Image')\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(mask_for_plot, cmap='gray')\n",
    "# plt.title('Mask')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
